{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ab80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# LSTM Seq2Seq para super-resolución de espectros (LR->HR)\n",
    "# Lectura directa desde HDF5 (streaming con h5py)\n",
    "# =========================================================\n",
    "import os, random, math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b727e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Configuración general\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "BATCH_SIZE = 256\n",
    "LR = 2e-3\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5              # Parada temprana (early stopping) para detener el entrenamiento cuando la perdida no es significante o retrocede\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "H5_PATH = \"fft_magnitude_norm_filtrada_65536_.h5\"     # Datasets: \"fft_mid\" (Low Resolution), \"fft_hr\" (High Resolution)\n",
    "X_KEY = \"fft_low\"\n",
    "Y_KEY = \"fft_hr\"\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Dataset HDF5 (streaming)\n",
    "# -----------------------------\n",
    "class H5SpectraDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Lee por índice directamente del archivo HDF5.\n",
    "    Aplica normalización min-max por muestra a X (LR) e Y (HR).\n",
    "    NOTA: use num_workers=0 en DataLoader para evitar problemas h5py.\n",
    "    \"\"\"\n",
    "    def __init__(self, h5_path, x_key=\"fft_mid\", y_key=\"fft_hr\",\n",
    "                 indices=None, normalize=True, eps=1e-12, return_index=False):\n",
    "        super().__init__()\n",
    "        self.h5_path = h5_path\n",
    "        self.x_key = x_key\n",
    "        self.y_key = y_key\n",
    "        self.normalize = normalize\n",
    "        self.eps = eps\n",
    "        self.return_index = return_index  # <-- NUEVO\n",
    "        self._file = None\n",
    "\n",
    "        with h5py.File(self.h5_path, \"r\") as f:\n",
    "            N = len(f[self.x_key])\n",
    "        all_idx = np.arange(N)\n",
    "        self.indices = all_idx if indices is None else np.asarray(indices, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.indices.size\n",
    "\n",
    "    def _ensure_open(self):\n",
    "        if self._file is None:\n",
    "            self._file = h5py.File(self.h5_path, \"r\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._ensure_open()\n",
    "        i = int(self.indices[idx])            # índice real en el H5\n",
    "        x = self._file[self.x_key][i]         # (L,)\n",
    "        y = self._file[self.y_key][i]         # (L,)\n",
    "\n",
    "        if self.normalize:\n",
    "            x_min, x_max = x.min(), x.max()\n",
    "            y_min, y_max = y.min(), y.max()\n",
    "            x = (x - x_min) / (x_max - x_min + self.eps)\n",
    "            y = (y - y_min) / (y_max - y_min + self.eps)\n",
    "\n",
    "        x = np.expand_dims(x.astype(np.float32), axis=-1)\n",
    "        y = np.expand_dims(y.astype(np.float32), axis=-1)\n",
    "        if self.return_index:                 # <-- NUEVO\n",
    "            return torch.from_numpy(x), torch.from_numpy(y), torch.tensor(i, dtype=torch.int64)\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            if self._file is not None:\n",
    "                self._file.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b548fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Separar conjuntos train/val/test (por índices)\n",
    "# -----------------------------\n",
    "with h5py.File(H5_PATH, \"r\") as f:\n",
    "    N = len(f[X_KEY])\n",
    "    N_2=f[X_KEY].shape[1]\n",
    "print(f\"Total muestras en {H5_PATH}: {N} con un largo de: {N_2} puntos\")\n",
    "\n",
    "train_ratio, val_ratio = 0.75, 0.20\n",
    "n_train = int(N * train_ratio)\n",
    "n_val   = int(N * val_ratio)\n",
    "n_test  = N - n_train - n_val\n",
    "\n",
    "all_indices = np.arange(N)\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(all_indices)\n",
    "train_idx = all_indices[:n_train]\n",
    "val_idx   = all_indices[n_train:n_train+n_val]\n",
    "test_idx  = all_indices[n_train+n_val:]\n",
    "\n",
    "train_ds = H5SpectraDataset(H5_PATH, X_KEY, Y_KEY, indices=train_idx, normalize=False)\n",
    "val_ds   = H5SpectraDataset(H5_PATH, X_KEY, Y_KEY, indices=val_idx,   normalize=False)\n",
    "test_ds  = H5SpectraDataset(H5_PATH, X_KEY, Y_KEY, indices=test_idx,  normalize=False, return_index=True)  \n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0bcec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_loader, val_loader, test_loader #elimina varibales para soltar el archivo y poder generarlo nuevamente\n",
    "# del train_ds, val_ds, test_ds             #elimina varibales para soltar el archivo y poder generarlo nuevamente\n",
    "# del iterable_test #elimina varibales para soltar el archivo y poder generarlo nuevamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d4a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnBridge(nn.Module):\n",
    "    def __init__(self, enc_dim, dec_dim, nhead=4, num_layers=1, dropout=0.1, ff_mult=2):\n",
    "        super().__init__()\n",
    "        self.proj_in = nn.Linear(enc_dim, dec_dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norms1 = nn.ModuleList([])\n",
    "        self.norms2 = nn.ModuleList([])\n",
    "        self.ff = nn.ModuleList([])\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(nn.MultiheadAttention(dec_dim, nhead, dropout=dropout, batch_first=True))\n",
    "            self.norms1.append(nn.LayerNorm(dec_dim))\n",
    "            self.ff.append(nn.Sequential(\n",
    "                nn.Linear(dec_dim, ff_mult*dec_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(ff_mult*dec_dim, dec_dim)\n",
    "            ))\n",
    "            self.norms2.append(nn.LayerNorm(dec_dim))\n",
    "\n",
    "    def forward(self, enc_out):  # enc_out: (B, L, enc_dim)\n",
    "        x = torch.relu(self.proj_in(enc_out))   # (B, L, dec_dim)\n",
    "        for mha, ln1, ffn, ln2 in zip(self.layers, self.norms1, self.ff, self.norms2):\n",
    "            y, _ = mha(x, x, x, need_weights=False)  # self-attn\n",
    "            x = ln1(x + y)\n",
    "            z = ffn(x)\n",
    "            x = ln2(x + z)\n",
    "        return x  # (B, L, dec_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6625d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSeq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder-Decoder LSTM (many-to-many alineado)\n",
    "    - Si bidirectional=True, el encoder es bidireccional y la capa bridge\n",
    "      reduce (2*enc_hidden) -> dec_hidden.\n",
    "    - El decoder recibe, por cada paso, la proyección z del encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_dim: int = 1,\n",
    "                 enc_hidden: int = 128,\n",
    "                 enc_layers: int = 2,\n",
    "                 dec_hidden: int | None = None,\n",
    "                 dec_layers: int = 2,\n",
    "                 dropout: float = 0.1,\n",
    "                 bidirectional: bool = True,\n",
    "                 use_attn_bridge=True, # Para activar el puente con atención\n",
    "                 attn_heads = 16, # Número de cabezas de atención\n",
    "                 attn_layers = 2# Número de capas de atención\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_dirs = 2 if bidirectional else 1\n",
    "        self.enc_hidden = enc_hidden\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=in_dim,\n",
    "            hidden_size=enc_hidden,\n",
    "            num_layers=enc_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if enc_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        # Dimensión de salida del encoder (concat de direcciones si es bidireccional)\n",
    "        enc_out_dim = enc_hidden * self.num_dirs\n",
    "\n",
    "        # Si no se especifica dec_hidden, igualarlo a enc_hidden\n",
    "        if dec_hidden is None:\n",
    "            dec_hidden = enc_hidden\n",
    "\n",
    "#----------INICIO ATENCION ---------------------------------------------------\n",
    "        # \"Puente\" para mapear la salida del encoder al espacio del decoder\n",
    "        #self.bridge = nn.Linear(enc_out_dim, dec_hidden) Se comenta para agregar codigo para puente con atención\n",
    "        if use_attn_bridge:\n",
    "            self.bridge = AttnBridge(enc_out_dim, dec_hidden,\n",
    "                                     nhead=attn_heads, num_layers=attn_layers, dropout=dropout)\n",
    "        else:\n",
    "            self.bridge = nn.Linear(enc_out_dim, dec_hidden)\n",
    "            nn.init.xavier_uniform_(self.bridge.weight)\n",
    "            nn.init.zeros_(self.bridge.bias)\n",
    "\n",
    "#----------FIN ATENCION ---------------------------------------------------\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=dec_hidden,\n",
    "            hidden_size=dec_hidden,\n",
    "            num_layers=dec_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if dec_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "\n",
    "        # Cabeza de predicción punto a punto\n",
    "        self.head = nn.Linear(dec_hidden, 1)\n",
    "\n",
    "        # Inicialización xavier para capas lineales\n",
    "        nn.init.xavier_uniform_(self.head.weight)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L, in_dim)\n",
    "        salida: (B, L, 1)\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        enc_out, _ = self.encoder(x)            # (B, L, enc_hidden * num_dirs)\n",
    "        \n",
    "#----------INICIO ATENCION ---------------------------------------------------\n",
    "\n",
    "        z = self.bridge(enc_out)    # (B, L, dec_hidden)\n",
    "\n",
    "#----------FIN ATENCION ---------------------------------------------------\n",
    "        # Decoder many-to-many alineado\n",
    "        dec_out, _ = self.decoder(z)            # (B, L, dec_hidden)\n",
    "        # Predicción por paso\n",
    "        y_hat = self.head(dec_out)              # (B, L, 1)\n",
    "        return y_hat\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ea408",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMSeq2Seq(\n",
    "                    in_dim=1, \n",
    "                    enc_hidden=128, \n",
    "                    enc_layers=2,\n",
    "                    dec_hidden=128, \n",
    "                    dec_layers=2, \n",
    "                    dropout=0.1\n",
    "                    ).to(DEVICE)\n",
    "\n",
    "criterion =nn.HuberLoss(delta=0.05)# nn.MSELoss()# nn.L1Loss()  # \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4, betas=(0.9, 0.98))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Entrenamiento con early stopping\n",
    "# -----------------------------\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "patience = PATIENCE\n",
    "history = {\"train\": [], \"val\": []}\n",
    "\n",
    "def _sync():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # ---- inicio de época ----\n",
    "    _sync()\n",
    "    t0 = time.time()\n",
    "    # Train----------------------------------------\n",
    "    model.train()\n",
    "    tr_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE, non_blocking=True).float()\n",
    "        yb = yb.to(DEVICE, non_blocking=True).float()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        yhat = model(xb)\n",
    "        loss = criterion(yhat, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item() * xb.size(0)\n",
    "    tr_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Val---------------------------------------------------------------\n",
    "    model.eval()\n",
    "    vl_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(DEVICE, non_blocking=True).float()\n",
    "            yb = yb.to(DEVICE, non_blocking=True).float()\n",
    "            yhat = model(xb)\n",
    "            vl_loss += criterion(yhat, yb).item() * xb.size(0)\n",
    "    vl_loss /= len(val_loader.dataset)\n",
    "\n",
    "    history[\"train\"].append(tr_loss)\n",
    "    history[\"val\"].append(vl_loss)\n",
    "    # ---- fin de época ----\n",
    "    _sync()\n",
    "    epoch_secs = time.time() - t0\n",
    "    eta = (EPOCHS - epoch) * epoch_secs  # ETA simple\n",
    "\n",
    "    print(f\"[{epoch:03d}] \"\n",
    "          f\"train {tr_loss:.6e} | val {vl_loss:.6e} | \"\n",
    "          f\"time {timedelta(seconds=int(epoch_secs))} | \"\n",
    "          f\"ETA {timedelta(seconds=int(eta))}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if vl_loss < best_val - 1e-7:\n",
    "        best_val = vl_loss\n",
    "        patience = PATIENCE\n",
    "        torch.save(model.state_dict(), \"best_lstm_seq2seq.pt\")\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# (Opcional) Curvas de pérdida\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history[\"train\"], label=\"train\")\n",
    "plt.plot(history[\"val\"], label=\"val\")\n",
    "plt.title(\"Curva de pérdida\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar el ultimo mejor modelo entrenado\n",
    "model.load_state_dict(torch.load(\"best_lstm_seq2seq.pt\", map_location=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c643a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Métricas y utilidades\n",
    "# -----------------------------\n",
    "def metrics_np(y_true, y_pred):\n",
    "    # y_true/pred: (N, L)\n",
    "    Yt = np.atleast_2d(y_true).astype(np.float64)  # (N, L)\n",
    "    Yp = np.atleast_2d(y_pred).astype(np.float64)\n",
    "\n",
    "    flat_t = Yt.ravel()\n",
    "    flat_p = Yp.ravel()\n",
    "    mse = mean_squared_error(flat_t, flat_p)\n",
    "    mae = mean_absolute_error(flat_t, flat_p)\n",
    "    r2  = r2_score(flat_t, flat_p)\n",
    "    # Pearson medio por muestra (sin scipy)\n",
    "    \n",
    "    A = Yt - Yt.mean(axis=1, keepdims=True)       # (N, L)\n",
    "    B = Yp - Yp.mean(axis=1, keepdims=True)       # (N, L)\n",
    "    num = np.sum(A * B, axis=1)                   # (N,)\n",
    "    den = np.sqrt(np.sum(A*A, axis=1) * np.sum(B*B, axis=1))  # (N,)\n",
    "\n",
    "    # Evitar NaN/inf cuando alguna varianza es 0: asigna 0 en esos casos\n",
    "    cors = np.divide(num, den, out=np.zeros_like(num), where=(den > 0))\n",
    "    \n",
    "    return {\n",
    "        \"MSE\": float(f\"{mse:.7f}\"),\n",
    "        \"MAE\": float(f\"{mae:.7f}\"),\n",
    "        \"R2\": float(f\"{r2:.6f}\"),\n",
    "        \"Pearson(mean)\": float(f\"{np.mean(cors):.6f}\")}\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluación en TEST (nunca visto)\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "Yt_list, Yp_list, Xt_list, I_list = [], [], [], []  # <-- añadimos I_list\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        xb, yb, ib = batch                       # <-- ahora vienen 3 tensores\n",
    "        xb = xb.to(DEVICE, non_blocking=True).float()\n",
    "        yb = yb.to(DEVICE, non_blocking=True).float()\n",
    "        yhat = model(xb)\n",
    "        Yt_list.append(yb.cpu().numpy())         # (B, L, 1)\n",
    "        Yp_list.append(yhat.cpu().numpy())       # (B, L, 1)\n",
    "        Xt_list.append(xb.cpu().numpy())         # (B, L, 1)\n",
    "        I_list.append(ib.cpu().numpy())          # (B,) índices reales H5\n",
    "\n",
    "Y_true = np.concatenate(Yt_list, axis=0)[:, :, 0]\n",
    "Y_pred = np.concatenate(Yp_list, axis=0)[:, :, 0]\n",
    "X_true = np.concatenate(Xt_list, axis=0)[:, :, 0]\n",
    "id_order = np.concatenate(I_list, axis=0)        # (N_test,) fila real H5 para cada muestra de Y_pred\n",
    "\n",
    "print(\"\\n== Métricas TEST ==\")\n",
    "print(metrics_np(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Graficar en eje real de frecuencia (Hz) usando spans efectivos\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "\n",
    "# Cargar metadatos para reconstrucción desde el mismo H5 del que se entrenó\n",
    "with h5py.File(H5_PATH, \"r\") as f:\n",
    "    spans_eff = f[\"fft_spans_eff\"][:]   # (M, 2) start,end tras recorte a 'upper'\n",
    "    kept_len  = f[\"kept_len\"][:]        # (M,)\n",
    "    freq_glob = f[\"fft_freq\"][:]        # (L_global,)\n",
    "    compounds = f[\"compounds\"][:]       # (M,)\n",
    "\n",
    "n_rows, n_cols = 14, 1\n",
    "num_to_plot = min(n_rows * n_cols, Y_pred.shape[0])\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(8, 60), sharex=False)\n",
    "\n",
    "# si axes es 1D, uniformiza índice\n",
    "axes_flat = axes.flat if hasattr(axes, \"flat\") else [axes]\n",
    "\n",
    "for i in range(num_to_plot):\n",
    "    ax = axes_flat[i]\n",
    "    irow = int(id_order[i])             # índice real en el H5\n",
    "    s, e = spans_eff[irow]              # bin start/end en el eje global\n",
    "    Lk   = int(kept_len[irow])          # longitud efectiva (sin padding)\n",
    "\n",
    "    # Eje real en Hz y series truncadas a Lk (evita zeros de padding)\n",
    "    x_hz   = freq_glob[s:e]\n",
    "    y_lr   = X_true[i, :Lk]\n",
    "    y_hr   = Y_true[i, :Lk]\n",
    "    y_pred = Y_pred[i, :Lk]\n",
    "\n",
    "    # nombre/fórmula\n",
    "    name = compounds[irow]\n",
    "    try:\n",
    "        name = name.decode()\n",
    "    except AttributeError:\n",
    "        name = str(name)\n",
    "\n",
    "\n",
    "    zoom = False\n",
    "    # zoom = True\n",
    "\n",
    "    # True]\n",
    "    if zoom:\n",
    "        zoom_min = 190.85  # kHz\n",
    "        zoom_max = 191.35  # kHz\n",
    "        mask = (x_hz/1000 >= zoom_min) & (x_hz/1000 <= zoom_max)\n",
    "        ax.set_xlim(zoom_min, zoom_max)\n",
    "    else:\n",
    "        mask = slice(None)  # todo el rango\n",
    "\n",
    "    ax.plot(x_hz[mask]/1000, y_lr[mask],   label=\"LR (input)\",   linewidth=0.8, alpha=0.8)\n",
    "    ax.plot(x_hz[mask]/1000, y_hr[mask],   label=\"HR (real)\",    linewidth=1.2)\n",
    "    ax.plot(x_hz[mask]/1000, y_pred[mask], label=\"HR (predicted)\",linewidth=1.0)\n",
    "  \n",
    "\n",
    "    ax.set_title(f\"{name}\\nMétricas: {metrics_np(y_hr, y_pred)}\")\n",
    "    ax.set_ylabel(\"Intensity (norm.)\")\n",
    "    ax.set_xlabel(\"Frequency (kHz)\")     # <-- eje real\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b06ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torchinfo\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "summary(model, input_size=(1024, 172, 1), dtypes=[torch.float32], col_names=(\n",
    "    \"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"\n",
    "))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
